{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression (1 feature)"
      ],
      "metadata": {
        "id": "qzfPOmyKvduy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imcNmFXhPdCh"
      },
      "source": [
        "# Import our standard libraries.\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style='darkgrid')  # default style\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VloE32t7dkU1"
      },
      "source": [
        "## Data for Linear Regression\n",
        "\n",
        "Suppose we have a dataset with 2 datapoints, $x^{(0)}$ and $x^{(1)}$. Each datapoint represents the value of a single feature. Because we'll want our model to have a learned *bias* (or *intercept*), we will use an extra feature which will always be set to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bcduWsAbCRl"
      },
      "source": [
        "# Here are our inputs.\n",
        "X = np.array([[1, 3],\n",
        "              [1, 2]])\n",
        "Y = np.array([7, 5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "print('X:', '\\n', X, '\\nX shape: ', X.shape, '\\n')\n",
        "print('Y:', '\\n', Y, '\\nY shape: ', Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFT5YpOEPq7s",
        "outputId": "79d05384-340c-4419-b5c4-a71a8ea2cde4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: \n",
            " [[1 3]\n",
            " [1 2]] \n",
            "X shape:  (2, 2) \n",
            "\n",
            "Y: \n",
            " [7 5] \n",
            "Y shape:  (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('X dimensions:', X.ndim)\n",
        "print('Y dimensions:', Y.ndim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RacrczRPQoam",
        "outputId": "ec17d26e-ac77-4cc7-a621-38a200f898d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X dimensions: 2\n",
            "Y dimensions: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSWDARd9eM99"
      },
      "source": [
        "## Example slicing\n",
        "Practice slicing the input array X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeQiqqBTd-t3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325112d7-d204-4546-f0ad-6098ba1b9516"
      },
      "source": [
        "# Use slicing to get the first X example (as an array)\n",
        "x_e0 = X[0,:]\n",
        "# Use slicing to get the second X example (as an array)\n",
        "x_e1 = X[1,:]\n",
        "\n",
        "x_e0, x_e1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1, 3]), array([1, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsuG8A-zeUV6"
      },
      "source": [
        "## Make predictions\n",
        "Suppose we are using linear regression. Then the functional form for our model is:\n",
        "\n",
        "\\begin{align}\n",
        "h(x) &= w_0 + w_1x\n",
        "\\end{align}\n",
        "\n",
        "Since we're using an extra feature corresponding to the intercept/bias, we can write:\n",
        "\n",
        "\\begin{align}\n",
        "h(x) &= w_0x_0 + w_1x_1 \\\\\n",
        "\\end{align}\n",
        "\n",
        "Given parameter values, practice computing model predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDbc2mGXcTvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "169c6c73-cb0d-44fe-dfdf-2b2c5c74ea4a"
      },
      "source": [
        "# Let's use a linear regression model: f(x) = w0 + w1*x1\n",
        "W = np.array([1, 1])\n",
        "print('W:', W, 'shape:', W.shape, 'dims: ', W.ndim)\n",
        "\n",
        "# Compute the prediction of the model for the first X example\n",
        "pred_e0 = np.dot(W, X[0])\n",
        "print('pred_e0: ', pred_e0)\n",
        "\n",
        "# Compute the prediction of the model for the second X example\n",
        "pred_e1 = np.dot(W, X[1])\n",
        "print('pred_e1: ', pred_e1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: [1 1] shape: (2,) dims:  1\n",
            "pred_e0:  4\n",
            "pred_e1:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NumPy: Dot Product Review\n",
        "\n",
        "Dot product of two arrays. Specifically,\n",
        "\n",
        "*   If both a and b are 1-D arrays, it is inner product of vectors\n",
        "*   If both a and b are 2-D arrays, it is matrix multiplication (*See class slides for more info*)\n",
        "\n",
        "[Refer to NumPy docs here](https://numpy.org/doc/stable/reference/generated/numpy.dot.html).\n",
        "\n",
        "\n",
        "The dot product between two vectors is a scalar and that only vectors with the same number of elements are compatible for a dot product.\n",
        "\n",
        "The dot product between a matrix x and a vector y returns a vector where the coefficients are the dot products between y and the rows of x.\n",
        "\n",
        "**Note**: When one of the two tensors has an ndim greater than 1, dot is no longer symmetric, which is to say that dot(x, y) isn’t the same as dot(y, x).\n",
        "\n",
        "Ref.: Francois Chollet, [see here](https://learning.oreilly.com/library/view/deep-learning-with/9781617296864/Text/02.htm#heading_id_20)\n"
      ],
      "metadata": {
        "id": "fAykAM2nWPDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see in more detail\n",
        "\n",
        "print('W: ', W)\n",
        "print('X[0]:', X[0])\n",
        "\n",
        "dot_product = np.dot(W, X[0])\n",
        "print(dot_product)  # Output: 4\n",
        "\n",
        "# Explanation:\n",
        "# (1 * 1) + (1 * 3) = 1 + 3 = 4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e46PC4pzTrFl",
        "outputId": "8f484ad4-33bd-43d5-ae1e-7df511b6409f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W:  [1 1]\n",
            "X[0]: [1 3]\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tdGfEoDovBm"
      },
      "source": [
        "Let's rewrite our model function with matrix multiplication and using the notation $h_W$ to make clear that our model is *parameterized* by $W$ (the vector of parameters):\n",
        "\n",
        "\\begin{align}\n",
        "h_W(x) = w_0x_0 + w_1x_1 = xW^T\n",
        "\\end{align}\n",
        "\n",
        "To make this matrix formulation as clear as possible, this is:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y} = h_W(x) = xW^T =\n",
        "\\begin{pmatrix}\n",
        "x_0 & x_1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "In addition, if we wanted to apply our model to *all* inputs $X$, we could simply use $XW^T$:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{Y} = h_W(X) = XW^T =\n",
        "\\begin{pmatrix}\n",
        "x_{0,0} & x_{0,1} \\\\\n",
        "x_{1,0} & x_{1,1} \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "x_{m-1,0} & x_{m-1,1} \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Remember that [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) requires the inner dimensions to line up:\n",
        "\n",
        "\\begin{align}\n",
        "X_{\\{m \\times n\\}} W^T_{\\{n \\times 1 \\}}  = \\hat{Y}_{\\{m \\times 1 \\}}\n",
        "\\end{align}\n",
        "\n",
        "Ok your turn.\n",
        "\n",
        "Use numpy functions to compute predictions for both X examples at once. The result should be a vector with 2 entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRyOr1S1oWAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc2f8ea5-547b-40a8-d70e-3bc57b6bba99"
      },
      "source": [
        "# Compute predictions for all X examples at once.\n",
        "\n",
        "print(X, '\\n------\\n',W)\n",
        "\n",
        "#X: 2x2\n",
        "#W.T: 2x1\n",
        "#Y_hat: 2x1\n",
        "preds = np.dot(X, W.T)\n",
        "print('preds:',preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 3]\n",
            " [1 2]] \n",
            "------\n",
            " [1 1]\n",
            "preds: [4 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8ZlvnYOpWxR"
      },
      "source": [
        "## Loss\n",
        "Use numpy functions to compute a vector of differences between model predictions and labels (values of Y)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UEd8aEbokKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1026ff33-1a65-4803-8a37-47515f9272d7"
      },
      "source": [
        "# Compute differences between predictions and labels.\n",
        "diff = preds - Y\n",
        "print(diff)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3 -2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J90USA7GqA2b"
      },
      "source": [
        "Now compute the MSE loss. The result should be a single (scalar) value. Remember we're using this formula (see assignment 1):\n",
        "\n",
        "\\begin{align}\n",
        "J(W) = \\frac{1}{2m} \\sum_{i=0}^{m-1} (h_W(x^{(i)}) - y^{(i)})^2\n",
        "\\end{align}\n",
        "\n",
        "where we've changed the standard scaling from $\\frac{1}{m}$ to $\\frac{1}{2m}$, where $m$ is the number of examples (2, in our case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcjG93R6qe-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed44a08-ea69-4ea8-a736-fdc35e6e05c0"
      },
      "source": [
        "# Get the number of examples\n",
        "m = X.shape[0]\n",
        "print('shape:', m)\n",
        "\n",
        "# Compute the average per-example loss\n",
        "loss = np.sum(diff**2) / m\n",
        "print('loss:', loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: 2\n",
            "loss: 6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWtjfoCZ5n1f"
      },
      "source": [
        "## Gradient\n",
        "\n",
        "Refer to assignment 1 or the gradient descent lecture for the derivation, but here's the formula for the partial derivatives (which together form the gradient):\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial w_j} J(W) &= (h_W(x) - y)x_j\n",
        "\\end{align}\n",
        "\n",
        "This formula is assuming we only have a single example. The general formula has an average over examples:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial w_j} J(W) &= \\frac{1}{m}\\sum_i(h_W(x^{(i)}) - y^{(i)})x^{(i)}_j\n",
        "\\end{align}\n",
        "\n",
        "You're ready to compute the gradient. The result will be a vector of partial derivatives for $w_0$ and $w_1$ respectively. You can use matrix computations as before.\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla J(W) &= \\frac{1}{m}(h_W(X) - Y)X\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwxFEpUr5q9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a754a2-bdaf-4008-e514-5bf9bfa78207"
      },
      "source": [
        "# compute the gradient\n",
        "gradient = np.dot(diff, X) / m\n",
        "print(gradient)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.5 -6.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpBEnP5sLZks"
      },
      "source": [
        "## Parameter updates\n",
        "Now that you've computed the gradient, apply parameter updates by subtracting the appropriate partial derivatives (scaled by a learning rate) from the initial parameter values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dtpDEhYZZwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb6521be-39b7-4146-bb28-85d840482bf2"
      },
      "source": [
        "# Update parameter values\n",
        "learning_rate = 0.1\n",
        "W = W - learning_rate*gradient\n",
        "print(W)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.25 1.65]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression (multiple features)\n",
        "\n",
        "Suppose we have a dataset with 2 datapoints, $x^{(0)}$ and $x^{(1)}$, but now we have multiple features."
      ],
      "metadata": {
        "id": "ff3KjlpLvsbG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVGGtopiv39T"
      },
      "source": [
        "# Here are our inputs.\n",
        "X = np.array([[1, 3, 1, 1],\n",
        "              [1, 2, 2, 0]])\n",
        "Y = np.array([2, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4bG4AeHv39T"
      },
      "source": [
        "Let's write out our model function:\n",
        "\n",
        "\\begin{align}\n",
        "h_W(x) = w_0x_0 + w_1x_1 + w_2x_2 + w_3x_3 = xW^T\n",
        "\\end{align}\n",
        "\n",
        "We can get all predictions with this matrix product:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{Y} = h_W(X) = XW^T =\n",
        "\\begin{pmatrix}\n",
        "x_{0,0} & x_{0,1} & x_{0,2} & x_{0,3} \\\\\n",
        "x_{1,0} & x_{1,1} & x_{1,2} & x_{1,3} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "x_{m-1,0} & x_{m-1,1} & x_{m-1,2} & x_{m-1,3} \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "w_2 \\\\\n",
        "w_3 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Given the (initial) parameter values below, compute the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGg1Ll4I4jR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc274435-5483-4b64-f0f7-b768efd02334"
      },
      "source": [
        "# Initial parameter values.\n",
        "W = [1, 1, 1, 1]\n",
        "\n",
        "# Compute predictions.\n",
        "preds = np.dot(X, W)\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ_pD-j2_qzV"
      },
      "source": [
        "Now compute the gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-Sam58B8B1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f1d2bd-e298-4120-899e-741ceacc968b"
      },
      "source": [
        "m, n = X.shape\n",
        "gradient = np.dot((preds - Y), X) / m\n",
        "print(gradient)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 4. 10.  6.  2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmYUmEjg_sk8"
      },
      "source": [
        "And now put everything together in gradient descent. You can run this cell repeatedly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl_Nu_wB8ar4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fce56ba-dc00-4bc9-b875-8cd63728ced4"
      },
      "source": [
        "# Run gradient descent\n",
        "learning_rate = 0.1\n",
        "\n",
        "preds = np.dot(X, W)\n",
        "loss = ((preds - Y)**2).mean()\n",
        "gradient = 2 * np.dot((preds - Y), X) / m\n",
        "W = W - learning_rate * gradient\n",
        "\n",
        "print('predictions:', preds)\n",
        "print('loss:', loss)\n",
        "print('gradient:', gradient)\n",
        "print('weights:', W)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions: [6 5]\n",
            "loss: 16.0\n",
            "gradient: [ 8. 20. 12.  4.]\n",
            "weights: [ 0.2 -1.  -0.2  0.6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMsSJ-ZD_12e"
      },
      "source": [
        "## Now with TensorFlow/Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jisaFtGY__KL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9659d3da-3344-4a03-8520-4b0f4ca6ab62"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(\n",
        "    units=1,                     # output dim\n",
        "    input_shape=[4],             # input dim\n",
        "    use_bias=False,              # we included the bias in X\n",
        "    kernel_initializer=tf.ones_initializer,  # initialize params to 1\n",
        "))\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "model.compile(loss='mse', optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PQ-RDwXCKVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8454eb9a-41a4-47b5-955c-d8cb9b920d20"
      },
      "source": [
        "history = model.fit(\n",
        "  x = X,\n",
        "  y = Y,\n",
        "  epochs=1,\n",
        "  batch_size=2,\n",
        "  verbose=0)\n",
        "loss = history.history['loss'][0]\n",
        "weights = model.layers[0].get_weights()[0].T\n",
        "preds = model.predict(X)\n",
        "\n",
        "print('predictions:', preds.T)\n",
        "print('loss:', loss)\n",
        "print('W:', weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "predictions: [[-2.4 -2.2]]\n",
            "loss: 16.0\n",
            "W: [[ 0.19999999 -1.         -0.20000005  0.6       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-QZEAyrD0pyz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}